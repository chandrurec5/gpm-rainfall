{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Neural network for Rain estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io as si\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data file path\n",
    "path =  '.'\n",
    "radar_guage_path = path + '/radar-guage-2009.mat'\n",
    "radar_trmm_path = path + '/radar-trmm-2009.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions will use in the data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class normalizer:\n",
    "    def __init__(self, m = 0.0, s = 1.0):\n",
    "        self.mean = m\n",
    "        self.std = s\n",
    "    def fit(self, st_data):\n",
    "        '''Only works for the 2D data, the 1st dimension is the number of the samples'''\n",
    "        self.mean = st_data.mean(axis = 0)\n",
    "        self.std = st_data.std(axis = 0)\n",
    "        print(self.mean, self.std)\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean)/self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_index(size):\n",
    "    length = int(size * 9/10)\n",
    "    random_arr = np.array(range(size))\n",
    "    np.random.shuffle(random_arr)\n",
    "    trainIndices = random_arr[:length]\n",
    "    valIndices = random_arr[length:]\n",
    "    print(trainIndices.shape,valIndices.shape)\n",
    "    return trainIndices,valIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1\n",
    "features : radar reflectivity profile\n",
    "\n",
    "label : rain rate (gauge)\n",
    "\n",
    "NN1 : reflectivity (4 inputs) -> rain estimation (1 output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### radar-gauge data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = si.loadmat(radar_guage_path)\n",
    "gauge = tmp['Yg']\n",
    "radar_reflectivity = np.transpose(tmp['xv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92772,) (10308,)\n"
     ]
    }
   ],
   "source": [
    "# simple partition to train and validate\n",
    "size = gauge.shape[0]\n",
    "length = int(size * 9/10)\n",
    "random_arr = np.array(range(size))\n",
    "np.random.shuffle(random_arr)\n",
    "trainIndices = random_arr[:length]\n",
    "valIndices = random_arr[length:]\n",
    "print(trainIndices.shape,valIndices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_data = radar_reflectivity[valIndices]\n",
    "validate_label = gauge[valIndices]\n",
    "train_data = radar_reflectivity[trainIndices]\n",
    "train_label = gauge[trainIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31.2263487   31.31230817  31.73983404  30.89607874] [ 7.77213029  7.23646837  6.32124218  5.82685087]\n"
     ]
    }
   ],
   "source": [
    "radar_nor = normalizer()\n",
    "radar_nor.fit(train_data)\n",
    "train_data_n = radar_nor.transform(train_data)\n",
    "validate_data_n = radar_nor.transform(validate_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92772, 4) (92772, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_n.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized radar reflectivity (4 inputs): \n",
      "[[-0.38310916 -0.45271033 -0.35754641  0.04698623]\n",
      " [ 1.15604422  1.06698824  1.03925776  0.0536044 ]\n",
      " [-0.33395831 -0.22622708 -0.06910984 -0.2338156 ]\n",
      " [ 1.06177119  1.31057741  1.34253245  0.63830815]\n",
      " [ 0.00959663 -0.07790809  0.15492103  0.12998119]]\n",
      "rain rate from gauage (1 output): \n",
      "[[  5.87008913]\n",
      " [ 20.11569968]\n",
      " [  5.11671747]\n",
      " [ 21.29256588]\n",
      " [  6.32942247]]\n"
     ]
    }
   ],
   "source": [
    "print('normalized radar reflectivity (4 inputs): ')\n",
    "print(train_data_n[:5])\n",
    "print('rain rate from gauage (1 output): ')\n",
    "print(train_label[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized radar reflectivity (4 inputs): \n",
      "[[-1.24952847 -1.40703575 -1.31211886 -1.41266253]\n",
      " [ 0.4494914   0.41691651  0.38445931  0.34758552]\n",
      " [-0.86540787 -0.92033975 -0.7042505  -0.26844187]\n",
      " [-0.67648256 -0.69276216 -0.23662907 -0.00331595]\n",
      " [ 0.27844698  0.1908006   0.33590332  0.84388009]]\n",
      "rain rate from gauage (1 output): \n",
      "[[ 2.02190931]\n",
      " [ 4.84542428]\n",
      " [ 5.05897411]\n",
      " [ 2.53822733]\n",
      " [ 6.09634956]]\n"
     ]
    }
   ],
   "source": [
    "print('normalized radar reflectivity (4 inputs): ')\n",
    "print(validate_data_n[:5])\n",
    "print('rain rate from gauage (1 output): ')\n",
    "print(validate_label[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# write create simple linear regression model\n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, 4))\n",
    "    tf_train_label = tf.placeholder(tf.float32, shape=(None,1))\n",
    "    tf_validate_dataset = tf.constant(validate_data_n, tf.float32)\n",
    "    tf_validate_label = tf.constant(validate_label, tf.float32)\n",
    "    weights = {'w1': tf.Variable(tf.truncated_normal([4, 16])),\n",
    "               'w2':tf.Variable(tf.truncated_normal([16, 4])),\n",
    "               'w3':tf.Variable(tf.truncated_normal([4, 1])),}\n",
    "    biases = {'b1':tf.Variable(tf.zeros([16])),\n",
    "              'b2':tf.Variable(tf.zeros([4])),\n",
    "              'b3':tf.Variable(tf.zeros([1])),}\n",
    "\n",
    "    def model(X, w, b):\n",
    "        hidden = tf.nn.relu(tf.matmul(X, w['w1']) + b['b1'])\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, w['w2']) + b['b2'])\n",
    "        return tf.matmul(hidden, w['w3']) + b['b3']\n",
    "\n",
    "    predicted_label = model(tf_train_dataset, weights, biases)\n",
    "    loss = tf.reduce_mean(tf.square(predicted_label - tf_train_label))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.005\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.95, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    train_loss = tf.reduce_mean(tf.abs(predicted_label - tf_train_label))\n",
    "    validate_pre = model(tf_validate_dataset, weights, biases)\n",
    "    validate_ame = tf.reduce_mean(tf.abs(validate_pre - tf_validate_label))\n",
    "    validate_rmse = tf.sqrt(tf.reduce_mean(tf.square(validate_pre - tf_validate_label)))\n",
    "    saver_nn1 = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-fc3c6f57772e>:6: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Loss at step 0: LR 0.005000 MSE 164.596848 MAE 6.917606\n",
      "AME = 6.219616, RMSE = 8.790132\n",
      "Loss at step 1000: LR 0.004512 MSE 40.457142 MAE 4.196101\n",
      "AME = 4.353083, RMSE = 6.286668\n",
      "Loss at step 2000: LR 0.004073 MSE 41.066116 MAE 4.434376\n",
      "AME = 4.340227, RMSE = 6.206257\n",
      "Loss at step 3000: LR 0.003675 MSE 35.310478 MAE 4.210016\n",
      "AME = 4.343867, RMSE = 6.183077\n",
      "Loss at step 4000: LR 0.003317 MSE 45.065392 MAE 4.434695\n",
      "AME = 4.309186, RMSE = 6.177742\n",
      "Loss at step 5000: LR 0.002994 MSE 36.186493 MAE 4.216193\n",
      "AME = 4.316359, RMSE = 6.183685\n",
      "Loss at step 6000: LR 0.002702 MSE 36.643585 MAE 4.563245\n",
      "AME = 4.390855, RMSE = 6.182939\n",
      "Loss at step 7000: LR 0.002438 MSE 47.621735 MAE 4.695786\n",
      "AME = 4.324302, RMSE = 6.174005\n",
      "Loss at step 8000: LR 0.002201 MSE 52.541786 MAE 5.067957\n",
      "AME = 4.289314, RMSE = 6.160225\n",
      "Loss at step 9000: LR 0.001986 MSE 35.454647 MAE 4.109489\n",
      "AME = 4.339703, RMSE = 6.168905\n",
      "Loss at step 10000: LR 0.001792 MSE 36.538246 MAE 3.917019\n",
      "AME = 4.272917, RMSE = 6.162662\n",
      "Loss at step 11000: LR 0.001618 MSE 36.293625 MAE 4.188137\n",
      "AME = 4.278274, RMSE = 6.159689\n",
      "Loss at step 12000: LR 0.001460 MSE 38.973022 MAE 4.392273\n",
      "AME = 4.335151, RMSE = 6.160952\n",
      "Loss at step 13000: LR 0.001318 MSE 42.845585 MAE 4.263504\n",
      "AME = 4.266096, RMSE = 6.157059\n",
      "Loss at step 14000: LR 0.001189 MSE 40.268951 MAE 4.297108\n",
      "AME = 4.261381, RMSE = 6.157315\n",
      "NN1.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the model with stochastic gradient descent training\n",
    "num_steps = 15000\n",
    "\n",
    "with tf.Session(graph = graph1) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_data_n[offset:(offset + batch_size), :]\n",
    "        #print(batch_data.shape)\n",
    "        batch_labels = train_label[offset:(offset + batch_size)]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_label : batch_labels}\n",
    "        #session.run(predicted_label, feed_dict=feed_dict)\n",
    "        _, l, _, r = session.run([predicted_label, loss, op, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: LR %f MSE %f MAE %f' % (step, r, l, train_loss.eval(feed_dict=feed_dict)))\n",
    "            print(\"AME = %f, RMSE = %f\"%(validate_ame.eval(), validate_rmse.eval()))\n",
    "            pre = validate_pre.eval()\n",
    "    \n",
    "            \n",
    "   \n",
    "    save_path = saver_nn1.save(session, \"NN1.ckpt\")\n",
    "    print(save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t      NN1.ckpt.index\t    radar-trmm-2009.mat\r\n",
      "NN1&2.ipynb\t\t      NN1.ckpt.meta\r\n",
      "NN1.ckpt.data-00000-of-00001  radar-guage-2009.mat\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2\n",
    "features : trmm reflectivity profile\n",
    "\n",
    "label : rain rate (radar)\n",
    "\n",
    "NN2 : trmm reflectivity (4 inputs) -> rain estimation (1 output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### radar-trmm data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = si.loadmat(radar_trmm_path)\n",
    "radar = np.transpose(tmp['radar'])\n",
    "trmm = np.transpose(tmp['trmm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7185, 4), (7185, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radar.shape, trmm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radar_n = radar_nor.transform(radar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph = graph1) as session:\n",
    "    saver_nn1.restore(session, \"./NN1.ckpt\")\n",
    "    feed_dict = {tf_train_dataset : radar_n}\n",
    "    radar_rr = predicted_label.eval(feed_dict = feed_dict)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6466,) (719,)\n"
     ]
    }
   ],
   "source": [
    "# simple partition to train and validate\n",
    "size = radar_rr.shape[0]\n",
    "Ind1, Ind2 = generate_random_index(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_data_nn2 = trmm[Ind2]\n",
    "validate_label_nn2 = radar_rr[Ind2]\n",
    "train_data_nn2 = trmm[Ind1]\n",
    "train_label_nn2 = radar_rr[Ind1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31.13711458  31.29384683  31.7139658   29.82048033] [ 6.67473408  6.6381645   6.39630846  6.23208654]\n"
     ]
    }
   ],
   "source": [
    "trmm_nor = normalizer()\n",
    "trmm_nor.fit(train_data_nn2)\n",
    "train_data_n2 = trmm_nor.transform(train_data_nn2)\n",
    "validate_data_n2 = trmm_nor.transform(validate_data_nn2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# write create simple linear regression model\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    tf_train_dataset2 = tf.placeholder(tf.float32, shape=(None, 4))\n",
    "    tf_train_label2 = tf.placeholder(tf.float32, shape=(None,1))\n",
    "    tf_validate_dataset2 = tf.constant(validate_data_n2, tf.float32)\n",
    "    tf_validate_label2 = tf.constant(validate_label_nn2, tf.float32)\n",
    "    weights2 = {'w1': tf.Variable(tf.truncated_normal([4, 16])),\n",
    "               'w2':tf.Variable(tf.truncated_normal([16, 4])),\n",
    "               'w3':tf.Variable(tf.truncated_normal([4, 1])),}\n",
    "    biases2 = {'b1':tf.Variable(tf.zeros([16])),\n",
    "              'b2':tf.Variable(tf.zeros([4])),\n",
    "              'b3':tf.Variable(tf.zeros([1])),}\n",
    "\n",
    "    def model(X, w, b):\n",
    "        hidden = tf.nn.relu(tf.matmul(X, w['w1']) + b['b1'])\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, w['w2']) + b['b2'])\n",
    "        return tf.matmul(hidden, w['w3']) + b['b3']\n",
    "\n",
    "    predicted_label2 = model(tf_train_dataset2, weights2, biases2)\n",
    "    loss2 = tf.reduce_mean(tf.square(predicted_label2 - tf_train_label2))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step2 = tf.Variable(0)\n",
    "    starter_learning_rate2 = 0.005\n",
    "    learning_rate2 = tf.train.exponential_decay(starter_learning_rate2, global_step2, 500, 0.95, staircase=True)\n",
    "    op2 = tf.train.GradientDescentOptimizer(learning_rate2).minimize(loss2, global_step = global_step2)\n",
    "    train_loss2 = tf.reduce_mean(tf.abs(predicted_label2 - tf_train_label2))\n",
    "    validate_pre2 = model(tf_validate_dataset2, weights2, biases2)\n",
    "    validate_ame2 = tf.reduce_mean(tf.abs(validate_pre2 - tf_validate_label2))\n",
    "    validate_rmse2 = tf.sqrt(tf.reduce_mean(tf.square(validate_pre2 - tf_validate_label2)))\n",
    "    saver_nn2 = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-2af17018b59d>:6: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Loss at step 0: LR 0.005000 MSE 143.426468 MAE 5.950020\n",
      "AME = 6.141518, RMSE = 7.322823\n",
      "Loss at step 1000: LR 0.004512 MSE 27.223122 MAE 2.907427\n",
      "AME = 2.847070, RMSE = 5.101861\n",
      "Loss at step 2000: LR 0.004073 MSE 25.671253 MAE 2.917471\n",
      "AME = 3.036306, RMSE = 5.007925\n",
      "Loss at step 3000: LR 0.003675 MSE 29.895626 MAE 3.418831\n",
      "AME = 2.804297, RMSE = 4.897387\n",
      "Loss at step 4000: LR 0.003317 MSE 30.132458 MAE 3.245851\n",
      "AME = 2.863469, RMSE = 4.892378\n",
      "Loss at step 5000: LR 0.002994 MSE 26.241253 MAE 3.039981\n",
      "AME = 2.827627, RMSE = 4.889169\n",
      "Loss at step 6000: LR 0.002702 MSE 29.613354 MAE 3.206232\n",
      "AME = 2.825691, RMSE = 4.889077\n",
      "Loss at step 7000: LR 0.002438 MSE 23.706402 MAE 2.838670\n",
      "AME = 2.895335, RMSE = 4.918851\n",
      "Loss at step 8000: LR 0.002201 MSE 23.810804 MAE 2.917967\n",
      "AME = 2.884224, RMSE = 4.901216\n",
      "Loss at step 9000: LR 0.001986 MSE 19.174904 MAE 2.775915\n",
      "AME = 2.804356, RMSE = 4.868639\n",
      "Loss at step 10000: LR 0.001792 MSE 32.338440 MAE 3.262286\n",
      "AME = 2.806664, RMSE = 4.873712\n",
      "Loss at step 11000: LR 0.001618 MSE 22.308775 MAE 2.604338\n",
      "AME = 2.920911, RMSE = 4.920659\n",
      "Loss at step 12000: LR 0.001460 MSE 25.668121 MAE 3.097432\n",
      "AME = 2.780282, RMSE = 4.868225\n",
      "Loss at step 13000: LR 0.001318 MSE 27.857105 MAE 3.208751\n",
      "AME = 2.812498, RMSE = 4.863184\n",
      "Loss at step 14000: LR 0.001189 MSE 20.259031 MAE 2.699212\n",
      "AME = 2.794545, RMSE = 4.863331\n",
      "NN2.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the model with stochastic gradient descent training\n",
    "num_steps = 15000\n",
    "\n",
    "with tf.Session(graph = graph2) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label_nn2.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_data_n2[offset:(offset + batch_size), :]\n",
    "        #print(batch_data.shape)\n",
    "        batch_labels = train_label_nn2[offset:(offset + batch_size)]\n",
    "        feed_dict = {tf_train_dataset2 : batch_data, tf_train_label2 : batch_labels}\n",
    "        #session.run(predicted_label, feed_dict=feed_dict)\n",
    "        _, l, _, r = session.run([predicted_label2, loss2, op2, learning_rate2], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: LR %f MSE %f MAE %f' % (step, r, l, train_loss2.eval(feed_dict=feed_dict)))\n",
    "            print(\"AME = %f, RMSE = %f\"%(validate_ame2.eval(), validate_rmse2.eval()))\n",
    "            pre = validate_pre2.eval()\n",
    "            \n",
    "    \n",
    "            \n",
    "   \n",
    "    save_path = saver_nn2.save(session, \"NN2.ckpt\")\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t      NN2.ckpt.data-00000-of-00001\r\n",
      "NN1&2.ipynb\t\t      NN2.ckpt.index\r\n",
      "NN1.ckpt.data-00000-of-00001  NN2.ckpt.meta\r\n",
      "NN1.ckpt.index\t\t      radar-guage-2009.mat\r\n",
      "NN1.ckpt.meta\t\t      radar-trmm-2009.mat\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
